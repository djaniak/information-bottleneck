{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append(\"../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"6\"\n",
    "\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_size_in_bytes(model):\n",
    "    \"\"\"\n",
    "    Calculate the size of the model in bytes.\n",
    "    \n",
    "    Args:\n",
    "    model: The PyTorch model.\n",
    "    \n",
    "    Returns:\n",
    "    int: Size of the model in bytes.\n",
    "    \"\"\"\n",
    "    total_size = 0\n",
    "    for param in model.parameters():\n",
    "        total_size += param.numel() * param.element_size()\n",
    "    return total_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import repitl.matrix_itl as itl\n",
    "\n",
    "from auto_gptq import AutoGPTQForCausalLM, BaseQuantizeConfig\n",
    "\n",
    "\n",
    "pretrained_model_dir = \"facebook/opt-125m\"\n",
    "quantized_model_dir = \"facebook_opt-125m-4bit-128g\"\n",
    "\n",
    "\n",
    "# os.makedirs(quantized_model_dir, exist_ok=True)\n",
    "def get_wikitext2(nsamples, seed, seqlen, model):\n",
    "    from datasets import load_dataset\n",
    "\n",
    "    traindata = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\", split=\"train\")\n",
    "    testdata = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\", split=\"test\")\n",
    "\n",
    "    from transformers import AutoTokenizer\n",
    "\n",
    "    try:\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model, use_fast=False)\n",
    "    except Exception:\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model, use_fast=True)\n",
    "    trainenc = tokenizer(\"\\n\\n\".join(traindata[\"text\"]), return_tensors=\"pt\")\n",
    "    testenc = tokenizer(\"\\n\\n\".join(testdata[\"text\"]), return_tensors=\"pt\")\n",
    "\n",
    "    import random\n",
    "\n",
    "    random.seed(seed)\n",
    "    np.random.seed(0)\n",
    "    torch.random.manual_seed(0)\n",
    "\n",
    "    traindataset = []\n",
    "    for _ in range(nsamples):\n",
    "        i = random.randint(0, trainenc.input_ids.shape[1] - seqlen - 1)\n",
    "        j = i + seqlen\n",
    "        inp = trainenc.input_ids[:, i:j]\n",
    "        attention_mask = torch.ones_like(inp)\n",
    "        traindataset.append({\"input_ids\": inp, \"attention_mask\": attention_mask})\n",
    "    return traindataset, testenc\n",
    "\n",
    "\n",
    "\n",
    "def normalize(R):\n",
    "    \"\"\"\n",
    "    Normalize the input matrix by subtracting the mean and dividing by the L2 norm.\n",
    "    From https://github.com/waltonfuture/Matrix-Entropy\n",
    "\n",
    "    Args:\n",
    "        R (torch.Tensor): Input matrix to be normalized.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: Normalized matrix.\n",
    "\n",
    "    \"\"\"\n",
    "    with torch.no_grad():\n",
    "        mean = R.mean(dim=0)\n",
    "        R = R - mean\n",
    "        norms = torch.norm(R, p=2, dim=1, keepdim=True)\n",
    "        R = R / norms\n",
    "    return R\n",
    "\n",
    "@torch.no_grad()\n",
    "def opt_eval(model, testenc, dev, seqlen=2048):\n",
    "    print(\"Evaluating ...\")\n",
    "\n",
    "    testenc = testenc.input_ids\n",
    "    nsamples = testenc.numel() // seqlen\n",
    "\n",
    "    use_cache = model.config.use_cache\n",
    "    model.config.use_cache = False\n",
    "    layers = model.model.decoder.layers\n",
    "\n",
    "    model.model.decoder.embed_tokens = model.model.decoder.embed_tokens.to(dev)\n",
    "    model.model.decoder.embed_positions = model.model.decoder.embed_positions.to(dev)\n",
    "    if hasattr(model.model.decoder, \"project_out\") and model.model.decoder.project_out:\n",
    "        model.model.decoder.project_out = model.model.decoder.project_out.to(dev)\n",
    "    if hasattr(model.model.decoder, \"project_in\") and model.model.decoder.project_in:\n",
    "        model.model.decoder.project_in = model.model.decoder.project_in.to(dev)\n",
    "    layers[0] = layers[0].to(dev)\n",
    "\n",
    "    dtype = next(iter(model.parameters())).dtype\n",
    "    inps = torch.zeros((nsamples, seqlen, model.config.hidden_size), dtype=dtype, device=dev)\n",
    "    cache = {\"i\": 0, \"attention_mask\": None}\n",
    "\n",
    "    class Catcher(nn.Module):\n",
    "        def __init__(self, module):\n",
    "            super().__init__()\n",
    "            self.module = module\n",
    "\n",
    "        def forward(self, inp, **kwargs):\n",
    "            inps[cache[\"i\"]] = inp\n",
    "            cache[\"i\"] += 1\n",
    "            cache[\"attention_mask\"] = kwargs[\"attention_mask\"]\n",
    "            raise ValueError\n",
    "\n",
    "    layers[0] = Catcher(layers[0])\n",
    "    for i in range(nsamples):\n",
    "        batch = testenc[:, (i * seqlen) : ((i + 1) * seqlen)].to(dev)\n",
    "        try:\n",
    "            model(batch)\n",
    "        except ValueError:\n",
    "            pass\n",
    "    layers[0] = layers[0].module\n",
    "\n",
    "    layers[0] = layers[0].cpu()\n",
    "    model.model.decoder.embed_tokens = model.model.decoder.embed_tokens.cpu()\n",
    "    model.model.decoder.embed_positions = model.model.decoder.embed_positions.cpu()\n",
    "    if hasattr(model.model.decoder, \"project_out\") and model.model.decoder.project_out:\n",
    "        model.model.decoder.project_out = model.model.decoder.project_out.cpu()\n",
    "    if hasattr(model.model.decoder, \"project_in\") and model.model.decoder.project_in:\n",
    "        model.model.decoder.project_in = model.model.decoder.project_in.cpu()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    outs = torch.zeros_like(inps)\n",
    "    attention_mask = cache[\"attention_mask\"]\n",
    "\n",
    "    for i in range(len(layers)):\n",
    "        # print(i)\n",
    "        layer = layers[i].to(dev)\n",
    "\n",
    "        for j in range(nsamples):\n",
    "            outs[j] = layer(inps[j].unsqueeze(0), attention_mask=attention_mask)[0]\n",
    "        layers[i] = layer.cpu()\n",
    "        del layer\n",
    "        torch.cuda.empty_cache()\n",
    "        inps, outs = outs, inps\n",
    "\n",
    "    if model.model.decoder.final_layer_norm is not None:\n",
    "        model.model.decoder.final_layer_norm = model.model.decoder.final_layer_norm.to(dev)\n",
    "    if model.model.decoder.project_out is not None:\n",
    "        model.model.decoder.project_out = model.model.decoder.project_out.to(dev)\n",
    "    model.lm_head = model.lm_head.to(dev)\n",
    "\n",
    "    testenc = testenc.to(dev)\n",
    "    nlls = []\n",
    "    ents = []\n",
    "\n",
    "    for i in range(nsamples):\n",
    "        hidden_states = inps[i].unsqueeze(0)\n",
    "        if model.model.decoder.final_layer_norm is not None:\n",
    "            hidden_states = model.model.decoder.final_layer_norm(hidden_states)\n",
    "        if model.model.decoder.project_out is not None:\n",
    "            hidden_states = model.model.decoder.project_out(hidden_states)\n",
    "\n",
    "        # perplexity\n",
    "        lm_logits = model.lm_head(hidden_states)\n",
    "        shift_logits = lm_logits[:, :-1, :].contiguous()\n",
    "        shift_labels = testenc[:, (i * seqlen) : ((i + 1) * seqlen)][:, 1:]\n",
    "        loss_fct = nn.CrossEntropyLoss()\n",
    "        loss = loss_fct(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))\n",
    "        neg_log_likelihood = loss.float() * seqlen\n",
    "        nlls.append(neg_log_likelihood)\n",
    "\n",
    "        # entropy\n",
    "        N, D = hidden_states.shape[1:]\n",
    "        hidden_states = normalize(hidden_states.squeeze())\n",
    "        if N > D:\n",
    "            cov = hidden_states.T @ hidden_states\n",
    "        else:\n",
    "            cov = hidden_states @ hidden_states.T\n",
    "        cov /= torch.trace(cov)\n",
    "        entropy = itl.matrixAlphaEntropy(cov.float(), alpha=1)\n",
    "        ents.append(entropy)\n",
    "\n",
    "    ppl = torch.exp(torch.stack(nlls).sum() / (nsamples * seqlen))\n",
    "    print(ppl.item())\n",
    "\n",
    "    ents = torch.stack(ents).cpu()\n",
    "    logD_normalized_entropy = ents / np.log(seqlen)\n",
    "    logN_normalized_entropy = ents / np.log(nsamples)\n",
    "    logNlogD_normalized_entropy = ents / (np.log(nsamples) * np.log(seqlen))\n",
    "\n",
    "    model.config.use_cache = use_cache\n",
    "\n",
    "    return {\n",
    "        \"ppl\": ppl.item(),\n",
    "        \"entropy\": ents.mean().item(),\n",
    "        \"logD_normalized_entropy\": logD_normalized_entropy.mean().item(),\n",
    "        \"logN_normalized_entropy\": logN_normalized_entropy.mean().item(),\n",
    "        \"logNlogD_normalized_entropy\": logNlogD_normalized_entropy.mean().item(),\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/djaniak/mambaforge/envs/info/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "traindataset, testenc = get_wikitext2(128, 0, 2048, pretrained_model_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[    2, 50140,  5457,  ...,  1437, 50140, 50118]])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testenc.input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/djaniak/mambaforge/envs/info/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "03fd1fd2dc364123af2c42d18f2921b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# from transformers import AutoModelForCausalLM\n",
    "\n",
    "\n",
    "# for pretrained_model_dir in [\n",
    "#     # \"facebook_opt-1.3b\",\n",
    "#     # \"facebook_opt-2.7b\",\n",
    "#     # \"facebook_opt-6.7b\",\n",
    "#     \"facebook_opt-13b\"\n",
    "#     ]:\n",
    "#     model = AutoModelForCausalLM.from_pretrained(pretrained_model_dir.replace(\"_\", \"/\"), device_map=\"auto\", torch_dtype=torch.float16)\n",
    "#     # model.save_pretrained(pretrained_model_dir)\n",
    "#     # model.save_pretrained(pretrained_model_dir, use_safetensors=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !cp -r ~/.cache/huggingface/hub/models--facebook--opt-13b ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - Start quantizing layer 1/12\n",
      "INFO:auto_gptq.modeling._base:Start quantizing layer 1/12\n",
      "INFO - Quantizing self_attn.k_proj in layer 1/12...\n",
      "INFO:auto_gptq.modeling._base:Quantizing self_attn.k_proj in layer 1/12...\n",
      "INFO - Quantizing self_attn.v_proj in layer 1/12...\n",
      "INFO:auto_gptq.modeling._base:Quantizing self_attn.v_proj in layer 1/12...\n",
      "INFO - Quantizing self_attn.q_proj in layer 1/12...\n",
      "INFO:auto_gptq.modeling._base:Quantizing self_attn.q_proj in layer 1/12...\n",
      "INFO - Quantizing self_attn.out_proj in layer 1/12...\n",
      "INFO:auto_gptq.modeling._base:Quantizing self_attn.out_proj in layer 1/12...\n",
      "INFO - Quantizing fc1 in layer 1/12...\n",
      "INFO:auto_gptq.modeling._base:Quantizing fc1 in layer 1/12...\n",
      "INFO - Quantizing fc2 in layer 1/12...\n",
      "INFO:auto_gptq.modeling._base:Quantizing fc2 in layer 1/12...\n",
      "INFO - Start quantizing layer 2/12\n",
      "INFO:auto_gptq.modeling._base:Start quantizing layer 2/12\n",
      "INFO - Quantizing self_attn.k_proj in layer 2/12...\n",
      "INFO:auto_gptq.modeling._base:Quantizing self_attn.k_proj in layer 2/12...\n",
      "INFO - Quantizing self_attn.v_proj in layer 2/12...\n",
      "INFO:auto_gptq.modeling._base:Quantizing self_attn.v_proj in layer 2/12...\n",
      "INFO - Quantizing self_attn.q_proj in layer 2/12...\n",
      "INFO:auto_gptq.modeling._base:Quantizing self_attn.q_proj in layer 2/12...\n",
      "INFO - Quantizing self_attn.out_proj in layer 2/12...\n",
      "INFO:auto_gptq.modeling._base:Quantizing self_attn.out_proj in layer 2/12...\n",
      "INFO - Quantizing fc1 in layer 2/12...\n",
      "INFO:auto_gptq.modeling._base:Quantizing fc1 in layer 2/12...\n",
      "INFO - Quantizing fc2 in layer 2/12...\n",
      "INFO:auto_gptq.modeling._base:Quantizing fc2 in layer 2/12...\n",
      "INFO - Start quantizing layer 3/12\n",
      "INFO:auto_gptq.modeling._base:Start quantizing layer 3/12\n",
      "INFO - Quantizing self_attn.k_proj in layer 3/12...\n",
      "INFO:auto_gptq.modeling._base:Quantizing self_attn.k_proj in layer 3/12...\n",
      "INFO - Quantizing self_attn.v_proj in layer 3/12...\n",
      "INFO:auto_gptq.modeling._base:Quantizing self_attn.v_proj in layer 3/12...\n",
      "INFO - Quantizing self_attn.q_proj in layer 3/12...\n",
      "INFO:auto_gptq.modeling._base:Quantizing self_attn.q_proj in layer 3/12...\n",
      "INFO - Quantizing self_attn.out_proj in layer 3/12...\n",
      "INFO:auto_gptq.modeling._base:Quantizing self_attn.out_proj in layer 3/12...\n",
      "INFO - Quantizing fc1 in layer 3/12...\n",
      "INFO:auto_gptq.modeling._base:Quantizing fc1 in layer 3/12...\n",
      "INFO - Quantizing fc2 in layer 3/12...\n",
      "INFO:auto_gptq.modeling._base:Quantizing fc2 in layer 3/12...\n",
      "INFO - Start quantizing layer 4/12\n",
      "INFO:auto_gptq.modeling._base:Start quantizing layer 4/12\n",
      "INFO - Quantizing self_attn.k_proj in layer 4/12...\n",
      "INFO:auto_gptq.modeling._base:Quantizing self_attn.k_proj in layer 4/12...\n",
      "INFO - Quantizing self_attn.v_proj in layer 4/12...\n",
      "INFO:auto_gptq.modeling._base:Quantizing self_attn.v_proj in layer 4/12...\n",
      "INFO - Quantizing self_attn.q_proj in layer 4/12...\n",
      "INFO:auto_gptq.modeling._base:Quantizing self_attn.q_proj in layer 4/12...\n",
      "INFO - Quantizing self_attn.out_proj in layer 4/12...\n",
      "INFO:auto_gptq.modeling._base:Quantizing self_attn.out_proj in layer 4/12...\n",
      "INFO - Quantizing fc1 in layer 4/12...\n",
      "INFO:auto_gptq.modeling._base:Quantizing fc1 in layer 4/12...\n",
      "INFO - Quantizing fc2 in layer 4/12...\n",
      "INFO:auto_gptq.modeling._base:Quantizing fc2 in layer 4/12...\n",
      "INFO - Start quantizing layer 5/12\n",
      "INFO:auto_gptq.modeling._base:Start quantizing layer 5/12\n",
      "INFO - Quantizing self_attn.k_proj in layer 5/12...\n",
      "INFO:auto_gptq.modeling._base:Quantizing self_attn.k_proj in layer 5/12...\n",
      "INFO - Quantizing self_attn.v_proj in layer 5/12...\n",
      "INFO:auto_gptq.modeling._base:Quantizing self_attn.v_proj in layer 5/12...\n",
      "INFO - Quantizing self_attn.q_proj in layer 5/12...\n",
      "INFO:auto_gptq.modeling._base:Quantizing self_attn.q_proj in layer 5/12...\n",
      "INFO - Quantizing self_attn.out_proj in layer 5/12...\n",
      "INFO:auto_gptq.modeling._base:Quantizing self_attn.out_proj in layer 5/12...\n",
      "INFO - Quantizing fc1 in layer 5/12...\n",
      "INFO:auto_gptq.modeling._base:Quantizing fc1 in layer 5/12...\n",
      "INFO - Quantizing fc2 in layer 5/12...\n",
      "INFO:auto_gptq.modeling._base:Quantizing fc2 in layer 5/12...\n",
      "INFO - Start quantizing layer 6/12\n",
      "INFO:auto_gptq.modeling._base:Start quantizing layer 6/12\n",
      "INFO - Quantizing self_attn.k_proj in layer 6/12...\n",
      "INFO:auto_gptq.modeling._base:Quantizing self_attn.k_proj in layer 6/12...\n",
      "INFO - Quantizing self_attn.v_proj in layer 6/12...\n",
      "INFO:auto_gptq.modeling._base:Quantizing self_attn.v_proj in layer 6/12...\n",
      "INFO - Quantizing self_attn.q_proj in layer 6/12...\n",
      "INFO:auto_gptq.modeling._base:Quantizing self_attn.q_proj in layer 6/12...\n",
      "INFO - Quantizing self_attn.out_proj in layer 6/12...\n",
      "INFO:auto_gptq.modeling._base:Quantizing self_attn.out_proj in layer 6/12...\n",
      "INFO - Quantizing fc1 in layer 6/12...\n",
      "INFO:auto_gptq.modeling._base:Quantizing fc1 in layer 6/12...\n",
      "INFO - Quantizing fc2 in layer 6/12...\n",
      "INFO:auto_gptq.modeling._base:Quantizing fc2 in layer 6/12...\n",
      "INFO - Start quantizing layer 7/12\n",
      "INFO:auto_gptq.modeling._base:Start quantizing layer 7/12\n",
      "INFO - Quantizing self_attn.k_proj in layer 7/12...\n",
      "INFO:auto_gptq.modeling._base:Quantizing self_attn.k_proj in layer 7/12...\n",
      "INFO - Quantizing self_attn.v_proj in layer 7/12...\n",
      "INFO:auto_gptq.modeling._base:Quantizing self_attn.v_proj in layer 7/12...\n",
      "INFO - Quantizing self_attn.q_proj in layer 7/12...\n",
      "INFO:auto_gptq.modeling._base:Quantizing self_attn.q_proj in layer 7/12...\n",
      "INFO - Quantizing self_attn.out_proj in layer 7/12...\n",
      "INFO:auto_gptq.modeling._base:Quantizing self_attn.out_proj in layer 7/12...\n",
      "INFO - Quantizing fc1 in layer 7/12...\n",
      "INFO:auto_gptq.modeling._base:Quantizing fc1 in layer 7/12...\n",
      "INFO - Quantizing fc2 in layer 7/12...\n",
      "INFO:auto_gptq.modeling._base:Quantizing fc2 in layer 7/12...\n",
      "INFO - Start quantizing layer 8/12\n",
      "INFO:auto_gptq.modeling._base:Start quantizing layer 8/12\n",
      "INFO - Quantizing self_attn.k_proj in layer 8/12...\n",
      "INFO:auto_gptq.modeling._base:Quantizing self_attn.k_proj in layer 8/12...\n",
      "INFO - Quantizing self_attn.v_proj in layer 8/12...\n",
      "INFO:auto_gptq.modeling._base:Quantizing self_attn.v_proj in layer 8/12...\n",
      "INFO - Quantizing self_attn.q_proj in layer 8/12...\n",
      "INFO:auto_gptq.modeling._base:Quantizing self_attn.q_proj in layer 8/12...\n",
      "INFO - Quantizing self_attn.out_proj in layer 8/12...\n",
      "INFO:auto_gptq.modeling._base:Quantizing self_attn.out_proj in layer 8/12...\n",
      "INFO - Quantizing fc1 in layer 8/12...\n",
      "INFO:auto_gptq.modeling._base:Quantizing fc1 in layer 8/12...\n",
      "INFO - Quantizing fc2 in layer 8/12...\n",
      "INFO:auto_gptq.modeling._base:Quantizing fc2 in layer 8/12...\n",
      "INFO - Start quantizing layer 9/12\n",
      "INFO:auto_gptq.modeling._base:Start quantizing layer 9/12\n",
      "INFO - Quantizing self_attn.k_proj in layer 9/12...\n",
      "INFO:auto_gptq.modeling._base:Quantizing self_attn.k_proj in layer 9/12...\n",
      "INFO - Quantizing self_attn.v_proj in layer 9/12...\n",
      "INFO:auto_gptq.modeling._base:Quantizing self_attn.v_proj in layer 9/12...\n",
      "INFO - Quantizing self_attn.q_proj in layer 9/12...\n",
      "INFO:auto_gptq.modeling._base:Quantizing self_attn.q_proj in layer 9/12...\n",
      "INFO - Quantizing self_attn.out_proj in layer 9/12...\n",
      "INFO:auto_gptq.modeling._base:Quantizing self_attn.out_proj in layer 9/12...\n",
      "INFO - Quantizing fc1 in layer 9/12...\n",
      "INFO:auto_gptq.modeling._base:Quantizing fc1 in layer 9/12...\n",
      "INFO - Quantizing fc2 in layer 9/12...\n",
      "INFO:auto_gptq.modeling._base:Quantizing fc2 in layer 9/12...\n",
      "INFO - Start quantizing layer 10/12\n",
      "INFO:auto_gptq.modeling._base:Start quantizing layer 10/12\n",
      "INFO - Quantizing self_attn.k_proj in layer 10/12...\n",
      "INFO:auto_gptq.modeling._base:Quantizing self_attn.k_proj in layer 10/12...\n",
      "INFO - Quantizing self_attn.v_proj in layer 10/12...\n",
      "INFO:auto_gptq.modeling._base:Quantizing self_attn.v_proj in layer 10/12...\n",
      "INFO - Quantizing self_attn.q_proj in layer 10/12...\n",
      "INFO:auto_gptq.modeling._base:Quantizing self_attn.q_proj in layer 10/12...\n",
      "INFO - Quantizing self_attn.out_proj in layer 10/12...\n",
      "INFO:auto_gptq.modeling._base:Quantizing self_attn.out_proj in layer 10/12...\n",
      "INFO - Quantizing fc1 in layer 10/12...\n",
      "INFO:auto_gptq.modeling._base:Quantizing fc1 in layer 10/12...\n",
      "INFO - Quantizing fc2 in layer 10/12...\n",
      "INFO:auto_gptq.modeling._base:Quantizing fc2 in layer 10/12...\n",
      "INFO - Start quantizing layer 11/12\n",
      "INFO:auto_gptq.modeling._base:Start quantizing layer 11/12\n",
      "INFO - Quantizing self_attn.k_proj in layer 11/12...\n",
      "INFO:auto_gptq.modeling._base:Quantizing self_attn.k_proj in layer 11/12...\n",
      "INFO - Quantizing self_attn.v_proj in layer 11/12...\n",
      "INFO:auto_gptq.modeling._base:Quantizing self_attn.v_proj in layer 11/12...\n",
      "INFO - Quantizing self_attn.q_proj in layer 11/12...\n",
      "INFO:auto_gptq.modeling._base:Quantizing self_attn.q_proj in layer 11/12...\n",
      "INFO - Quantizing self_attn.out_proj in layer 11/12...\n",
      "INFO:auto_gptq.modeling._base:Quantizing self_attn.out_proj in layer 11/12...\n",
      "INFO - Quantizing fc1 in layer 11/12...\n",
      "INFO:auto_gptq.modeling._base:Quantizing fc1 in layer 11/12...\n",
      "INFO - Quantizing fc2 in layer 11/12...\n",
      "INFO:auto_gptq.modeling._base:Quantizing fc2 in layer 11/12...\n",
      "INFO - Start quantizing layer 12/12\n",
      "INFO:auto_gptq.modeling._base:Start quantizing layer 12/12\n",
      "INFO - Quantizing self_attn.k_proj in layer 12/12...\n",
      "INFO:auto_gptq.modeling._base:Quantizing self_attn.k_proj in layer 12/12...\n",
      "INFO - Quantizing self_attn.v_proj in layer 12/12...\n",
      "INFO:auto_gptq.modeling._base:Quantizing self_attn.v_proj in layer 12/12...\n",
      "INFO - Quantizing self_attn.q_proj in layer 12/12...\n",
      "INFO:auto_gptq.modeling._base:Quantizing self_attn.q_proj in layer 12/12...\n",
      "INFO - Quantizing self_attn.out_proj in layer 12/12...\n",
      "INFO:auto_gptq.modeling._base:Quantizing self_attn.out_proj in layer 12/12...\n",
      "INFO - Quantizing fc1 in layer 12/12...\n",
      "INFO:auto_gptq.modeling._base:Quantizing fc1 in layer 12/12...\n",
      "INFO - Quantizing fc2 in layer 12/12...\n",
      "INFO:auto_gptq.modeling._base:Quantizing fc2 in layer 12/12...\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "quantize_config = BaseQuantizeConfig(\n",
    "    bits=4,  # quantize model to 4-bit\n",
    "    group_size=128,  # it is recommended to set the value to 128\n",
    "    desc_act=False,  # desc_act and group size only works on triton\n",
    ")\n",
    "\n",
    "# load un-quantized model, the model will always be force loaded into cpu\n",
    "model = AutoGPTQForCausalLM.from_pretrained(pretrained_model_dir, quantize_config)\n",
    "\n",
    "# quantize model, the examples should be list of dict whose keys can only be \"input_ids\" and \"attention_mask\"\n",
    "# with value under torch.LongTensor type.\n",
    "model.quantize(traindataset, use_triton=False)\n",
    "  \n",
    "# save quantized model\n",
    "model.save_quantized(quantized_model_dir)\n",
    "\n",
    "# save quantized model using safetensors\n",
    "model.save_quantized(quantized_model_dir, use_safetensors=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING - Exllamav2 kernel is not installed, reset disable_exllamav2 to True. This may because you installed auto_gptq using a pre-build wheel on Windows, in which exllama_kernels are not compiled. To use exllama_kernels to further speedup inference, you can re-install auto_gptq from source.\n",
      "WARNING - CUDA kernels for auto_gptq are not installed, this will result in very slow inference speed. This may because:\n",
      "1. You disabled CUDA extensions compilation by setting BUILD_CUDA_EXT=0 when install auto_gptq from source.\n",
      "2. You are using pytorch without CUDA support.\n",
      "3. CUDA and nvcc are not installed in your device.\n",
      "WARNING - ignoring unknown parameter in quantize_config.json: quant_method.\n",
      "INFO - The layer lm_head is not quantized.\n"
     ]
    }
   ],
   "source": [
    "# load quantized model, currently only support cpu or single gpu\n",
    "model = AutoGPTQForCausalLM.from_quantized(quantized_model_dir, device_map=\"auto\", use_triton=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[    2, 50140,  5457,  ...,  1437, 50140, 50118]]), 'attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1]])}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testenc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating ...\n",
      "29.53990936279297\n"
     ]
    }
   ],
   "source": [
    "out = opt_eval(model.model, testenc, dev=\"cuda:0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ppl': 29.53990936279297,\n",
       " 'entropy': 5.074114799499512,\n",
       " 'logD_normalized_entropy': 0.6654909253120422,\n",
       " 'logN_normalized_entropy': 1.026807427406311,\n",
       " 'logNlogD_normalized_entropy': 0.13467001914978027,\n",
       " 'model': 'model'}"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out | {\"model\": \"model\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/djaniak/mambaforge/envs/info/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating ...\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "27.655488967895508\n"
     ]
    }
   ],
   "source": [
    "# load quantized model, currently only support cpu or single gpu\n",
    "model_ = AutoModelForCausalLM.from_pretrained( \"facebook/opt-125m\", device_map=\"auto\", torch_dtype=torch.float16)\n",
    "\n",
    "opt_eval(model_, testenc, dev=\"cuda:0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "250478592"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_size_in_bytes(model_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "info",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
