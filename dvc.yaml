stages:

  compute_opt_entropy_ppl_gptq:
    matrix:
      pretrained_model_dir: [
        "facebook_opt-1.3b",
        "facebook_opt-2.7b",
        "facebook_opt-6.7b",
        "facebook_opt-13b",
      ]
      bits: [ 2, 3, 4, 8, 16 ]
    cmd: >-
      PYTHONPATH=. python experiments/scripts/compute_opt_entropy_ppl_gptq.py
      --pretrained-model-dir ${item.pretrained_model_dir} --bits ${item.bits} --no-quantize
      --output-path data/entropies/gptq-wikitext2/${item.pretrained_model_dir}-${item.bits}bit.json
    outs:
      - data/entropies/gptq-wikitext2/${item.pretrained_model_dir}-${item.bits}bit.json:
          push: false

  evaluate_lms:
    matrix:
      model: [
        "facebook_opt-1.3b",
        "facebook_opt-2.7b",
        "facebook_opt-6.7b",
        "facebook_opt-13b",
      ]
      task: [ "openbookqa", "winogrande", "piqa", "hellaswag" ]
      quantization: [ "float16", "int4", "int8" ]
    cmd: >-
      PYTHONPATH=. python experiments/scripts/evaluate_llm.py 
      --model-name ${item.model} --task ${item.task} --quantization ${item.quantization} 
      --output-path data/lm_eval/${item.task}/${item.model}/${item.quantization}.json
    outs:
      - data/lm_eval/${item.task}/${item.model}/${item.quantization}.json
