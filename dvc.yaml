stages:
  compute_entropies:
    matrix:
      model: [
        "EleutherAI_pythia-14m",
        "EleutherAI_pythia-70m",
        "EleutherAI_pythia-160m",
        "EleutherAI_pythia-410m",
        "EleutherAI_pythia-1b",
        "EleutherAI_pythia-1.4b",
        "EleutherAI_pythia-2.8b",
      ]
      dataset: [ "wikitext" ]
      quantization: [ "none-float16", "bnb-int4", "bnb-int8", "quanto-int2", "quanto-int4", "quanto-int8" ]
    cmd: >-
      PYTHONPATH=. python experiments/scripts/compute_entropies.py 
      --model-name ${item.model} --dataset ${item.dataset} --quantization ${item.quantization} 
      --output-path data/entropies/${item.dataset}/${item.model}/${item.quantization}.json
    outs:
      - data/entropies/${item.dataset}/${item.model}/${item.quantization}.json:
          push: false

  evaluate_lms:
    matrix:
      model: [
        "EleutherAI_pythia-14m",
        "EleutherAI_pythia-70m",
        "EleutherAI_pythia-160m",
        "EleutherAI_pythia-410m",
        "EleutherAI_pythia-1b",
        "EleutherAI_pythia-2.8b",
      ]
      task: [ "openbookqa", "winogrande", "piqa", "hellaswag" ]
      quantization: [ "float16", "int4", "int8" ]
    cmd: >-
      PYTHONPATH=. python experiments/scripts/evaluate_llm.py 
      --model-name ${item.model} --task ${item.task} --quantization ${item.quantization} 
      --output-path data/lm_eval/${item.task}/${item.model}/${item.quantization}.json
    deps:
      - experiments/scripts/evaluate_llm.py
    outs:
      - data/lm_eval/${item.task}/${item.model}/${item.quantization}.json
